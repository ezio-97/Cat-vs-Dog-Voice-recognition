{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7706f629",
   "metadata": {},
   "source": [
    "* Compile, Train and Save the models here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01926d77",
   "metadata": {},
   "source": [
    "* 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d6e8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # CSV file\n",
    "import config\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a609b9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scale_and_PCALDA(path):\n",
    "\n",
    "    data = pd.read_csv(path)\n",
    "    num_columns = data.shape[1]\n",
    "    print(f\"Num of Columns is {num_columns}\")\n",
    "    X = np.array(data.iloc[:,0:num_columns-1])\n",
    "    y = np.array(data.iloc[:,num_columns-1])\n",
    "    # print(len(X[0]))\n",
    "    # print(y[0])\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)  # X shape: (n_samples, 12)\n",
    "\n",
    "    ''' PCA '''\n",
    "    n_components = 2\n",
    "    pca_object = PCA(n_components= n_components)\n",
    "    pca_object.fit(X_scaled)\n",
    "    PrincipleComps = pca_object.transform(X_scaled)\n",
    "    classes = np.unique(y)\n",
    "\n",
    "    for i in range(n_components):\n",
    "        plt.figure()\n",
    "        for clss in classes:\n",
    "            plt.hist(PrincipleComps[y == clss, i],\n",
    "                    bins=\"auto\", alpha=0.5, \n",
    "                    label=f\"Class {clss}\")\n",
    "        plt.xlabel(\"Feature intervals\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.title(f\"PCA by Class for feature column {i}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    score = davies_bouldin_score(PrincipleComps, y)\n",
    "    print(f\"The davies_bouldin_score for PCA is {score}\")\n",
    "\n",
    "\n",
    "    \"\"\" LDA \"\"\"\n",
    "\n",
    "    lda_mcc = LDA()\n",
    "    lda_mcc.fit(X_scaled,y)\n",
    "    lda_OP = lda_mcc.transform(X_scaled)\n",
    "    plt.figure()\n",
    "    for c in classes:\n",
    "        plt.hist(lda_OP[y == c], bins=20, alpha=0.5, label=f\"Class {c}\")\n",
    "    plt.xlabel(\"1D LDA Projection\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"LDA Projection onto First Component. 0 is cat, 1 is Dog\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3eed159",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Original DATA\")\n",
    "Scale_and_PCALDA(path = config.Features + 'data.csv' )\n",
    "\n",
    "print(\"fs300_cc12\")\n",
    "Scale_and_PCALDA(path = config.Features + 'data_fs300_cc12.csv')\n",
    "\n",
    "print(\"fs300_cc20\")\n",
    "Scale_and_PCALDA(path = config.Features + 'data_fs300_cc20.csv')\n",
    "\n",
    "print(\"fs300_cc30\")\n",
    "Scale_and_PCALDA(path = config.Features + 'data_fs300_cc30.csv')\n",
    "\n",
    "print(\"fs500_cc20\")\n",
    "Scale_and_PCALDA(path = config.Features + 'data_fs500_cc20.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbac18a",
   "metadata": {},
   "source": [
    "* 2. Training the model\n",
    "\n",
    "* First approach- Vanilla NN \n",
    "* fs300_cc20 looks good. Let us see...................."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b340965",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(config.Features + 'data_fs300_cc20.csv')\n",
    "num_columns = data.shape[1]\n",
    "X = np.array(data.iloc[:,0:num_columns-1])\n",
    "y = np.array(data.iloc[:,num_columns-1])\n",
    "# print(len(X[0]))\n",
    "# print(y[0])\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X) \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y,\n",
    "    test_size=0.2,        \n",
    "    stratify=y,           \n",
    "    random_state=42        \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b73752",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential([\n",
    "    Dense(64, input_shape=(20,), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Dense(32, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Dense(1, activation='sigmoid')  # Binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_split=0.2,\n",
    "                    epochs=50,\n",
    "                    batch_size=32,\n",
    "                    callbacks=[early_stop],\n",
    "                    verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0149072a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab369a8e",
   "metadata": {},
   "source": [
    "* Second approach- Vanilla NN and fs500_cc20 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c2185e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(config.Features + 'data_fs500_cc20.csv')\n",
    "num_columns = data.shape[1]\n",
    "X = np.array(data.iloc[:,0:num_columns-1])\n",
    "y = np.array(data.iloc[:,num_columns-1])\n",
    "# print(len(X[0]))\n",
    "# print(y[0])\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y,\n",
    "    test_size=0.2,         \n",
    "    stratify=y,            \n",
    "    random_state=42       \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14873d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Dense(64, input_shape=(20,), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Dense(32, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Dense(1, activation='sigmoid')  \n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_split=0.2,\n",
    "                    epochs=80,\n",
    "                    batch_size=32,\n",
    "                    callbacks=[early_stop],\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780c2174",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608f0fca",
   "metadata": {},
   "source": [
    "* Approach 3 - 1D convolutional model and fs300cc12 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dd488b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(config.Features + 'data_fs300_cc20.csv')\n",
    "num_columns = data.shape[1]\n",
    "X = np.array(data.iloc[:,0:num_columns-1])\n",
    "y = np.array(data.iloc[:,num_columns-1])\n",
    "# print(len(X[0]))\n",
    "# print(y[0])\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X) \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y,\n",
    "    test_size=0.2,         \n",
    "    stratify=y,            \n",
    "    random_state=42        \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39aa1396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input shape\n",
    "num_mfcc = 20\n",
    "num_frames = 50  # adjust depending on your data\n",
    "input_shape = (num_frames, num_mfcc)\n",
    "\n",
    "model = Sequential([\n",
    "    Conv1D(32, kernel_size=3, activation='relu', input_shape=input_shape),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Conv1D(64, kernel_size=3, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')  # Binary classification\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e93d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5827e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
